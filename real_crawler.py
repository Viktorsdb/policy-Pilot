#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urljoin
import os

class RealPolicyCrawler:
    """åŸºäºçœŸå®é“¾æ¥çš„æ”¿ç­–çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # çœŸå®æ”¿ç­–é“¾æ¥åˆ—è¡¨
        self.policy_urls = [
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c0692292eab01934dd52e3d09b9",
                "title": "å¾æ±‡åŒºå…³äºæ¨åŠ¨äººå·¥æ™ºèƒ½äº§ä¸šé«˜è´¨é‡å‘å±•çš„è‹¥å¹²æ„è§",
                "category": "ai_development",
                "region": "å¾æ±‡åŒº"
            },
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c0692292eab01934dd6cfbd09bb", 
                "title": "å¾æ±‡åŒºå…³äºæ¨åŠ¨å…·èº«æ™ºèƒ½äº§ä¸šå‘å±•çš„è‹¥å¹²æ„è§",
                "category": "embodied_ai",
                "region": "å¾æ±‡åŒº"
            },
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c0692292eab019384dc95a70aed",
                "title": "å…³äºæ”¯æŒä¸Šæµ·å¸‚ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆ›æ–°ç”Ÿæ€å…ˆå¯¼åŒºçš„è‹¥å¹²æªæ–½",
                "category": "generative_ai",
                "region": "å¾æ±‡åŒº"
            },
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c0692292eab0192e1617de30643",
                "title": "å¾æ±‡åŒºå…³äºæ”¯æŒè¥¿å²¸äººå·¥æ™ºèƒ½å¤§æ¨¡å‹ç§‘åˆ›è¡—åŒºçš„æ‰¶æŒæ„è§",
                "category": "ai_model_district",
                "region": "å¾æ±‡åŒº"
            },
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c0696c75c4d0195d6c9b66e0051",
                "title": "ä¿éšœæ€§å®‰å±…å·¥ç¨‹ä¸“é¡¹è¡¥åŠ©èµ„é‡‘åˆ†é…ç»“æœ",
                "category": "housing_subsidy",
                "region": "å¾æ±‡åŒº"
            },
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c0692292eab019361d266e109fc",
                "title": "å¾æ±‡åŒºæ–°å‹å·¥ä¸šåŒ–å‘å±•ä¸“é¡¹èµ„é‡‘ç®¡ç†åŠæ³•",
                "category": "industrial_development",
                "region": "å¾æ±‡åŒº"
            },
            {
                "url": "https://www.xuhui.gov.cn/xxgk/portal/article/detail?id=8a4c0c06920eedbe0192235ebcb3003e",
                "title": "2024å¹´å¾æ±‡åŒºå°å¾®ä¼ä¸šè´´æ¯è´´è´¹å®¡æ ¸é€šè¿‡åå•",
                "category": "sme_subsidy",
                "region": "å¾æ±‡åŒº"
            }
        ]
        
    def crawl_policy_content(self, url: str) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªæ”¿ç­–é¡µé¢å†…å®¹"""
        try:
            print(f"ğŸ” æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = self.extract_title(soup)
            
            # æå–æ­£æ–‡å†…å®¹
            content = self.extract_content(soup)
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_date = self.extract_publish_date(soup)
            
            # æå–æ¥æºéƒ¨é—¨
            department = self.extract_department(soup)
            
            # åˆ†ææ”¿ç­–ç±»å‹å’Œæ”¯æŒé‡‘é¢
            policy_type, max_amount = self.analyze_policy_type(content)
            
            # æå–ç”³è¯·æ¡ä»¶
            requirements = self.extract_requirements(content)
            
            # æå–è¡Œä¸šæ ‡ç­¾
            industry_tags = self.extract_industry_tags(title, content)
            
            return {
                "url": url,
                "title": title,
                "content": content,
                "publish_date": publish_date,
                "department": department,
                "policy_type": policy_type,
                "max_amount": max_amount,
                "requirements": requirements,
                "industry_tags": industry_tags,
                "crawl_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ {url}: {str(e)}")
            return None
    
    def extract_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
        selectors = [
            'h1.article-title',
            'h1',
            '.article-header h1',
            '.content-title h1',
            'title'
        ]
        
        for selector in selectors:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.get_text().strip():
                title = title_elem.get_text().strip()
                # æ¸…ç†æ ‡é¢˜
                if '|' in title:
                    title = title.split('|')[0].strip()
                if '-' in title and len(title.split('-')) > 1:
                    title = title.split('-')[0].strip()
                return title
        
        return "æœªçŸ¥æ”¿ç­–æ ‡é¢˜"
    
    def extract_content(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ­£æ–‡å†…å®¹"""
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        selectors = [
            '.article-content',
            '.content-main',
            '.policy-content',
            '.article-body',
            '#article-content',
            '.text-content'
        ]
        
        for selector in selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in content_elem(["script", "style"]):
                    script.decompose()
                
                content = content_elem.get_text()
                # æ¸…ç†æ–‡æœ¬
                content = re.sub(r'\s+', ' ', content).strip()
                if len(content) > 100:  # ç¡®ä¿å†…å®¹æœ‰æ„ä¹‰
                    return content
        
        # å¦‚æœä¸“ç”¨é€‰æ‹©å™¨å¤±è´¥ï¼Œå°è¯•è·å–æ•´ä¸ªé¡µé¢æ–‡æœ¬
        full_text = soup.get_text()
        full_text = re.sub(r'\s+', ' ', full_text).strip()
        return full_text[:2000] if full_text else "æ— æ³•æå–å†…å®¹"
    
    def extract_publish_date(self, soup: BeautifulSoup) -> str:
        """æå–å‘å¸ƒæ—¶é—´"""
        # å°è¯•å¤šç§æ—¶é—´é€‰æ‹©å™¨
        selectors = [
            '.publish-date',
            '.article-date',
            '.date',
            '.time',
            '[class*="date"]',
            '[class*="time"]'
        ]
        
        for selector in selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text().strip()
                # æå–æ—¥æœŸæ ¼å¼
                date_match = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', date_text)
                if date_match:
                    return date_match.group(1).replace('/', '-').replace('.', '-')
        
        # åœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢æ—¥æœŸ
        page_text = soup.get_text()
        date_match = re.search(r'(\d{4}[-./]\d{1,2}[-./]\d{1,2})', page_text)
        if date_match:
            return date_match.group(1).replace('/', '-').replace('.', '-')
        
        return "2024-01-01"  # é»˜è®¤æ—¥æœŸ
    
    def extract_department(self, soup: BeautifulSoup) -> str:
        """æå–å‘å¸ƒéƒ¨é—¨"""
        # åœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢éƒ¨é—¨ä¿¡æ¯
        page_text = soup.get_text()
        
        # å¸¸è§éƒ¨é—¨åç§°æ¨¡å¼
        dept_patterns = [
            r'(å¾æ±‡åŒº[^ï¼Œã€‚]+?å§”å‘˜ä¼š)',
            r'(å¾æ±‡åŒº[^ï¼Œã€‚]+?å±€)',
            r'(å¾æ±‡åŒº[^ï¼Œã€‚]+?åŠ)',
            r'(ä¸Šæµ·å¸‚[^ï¼Œã€‚]+?å§”å‘˜ä¼š)',
            r'(ä¸Šæµ·å¸‚[^ï¼Œã€‚]+?å±€)'
        ]
        
        for pattern in dept_patterns:
            match = re.search(pattern, page_text)
            if match:
                return match.group(1)
        
        # å¦‚æœåœ¨å¾æ±‡åŒºç½‘ç«™ï¼Œé»˜è®¤è¿”å›å¾æ±‡åŒºæ”¿åºœ
        if 'xuhui.gov.cn' in soup.find('base', href=True).get('href', '') if soup.find('base', href=True) else '':
            return "å¾æ±‡åŒºäººæ°‘æ”¿åºœ"
        
        return "å¾æ±‡åŒºç›¸å…³éƒ¨é—¨"
    
    def analyze_policy_type(self, content: str) -> tuple:
        """åˆ†ææ”¿ç­–ç±»å‹å’Œæœ€å¤§æ”¯æŒé‡‘é¢"""
        content_lower = content.lower()
        
        # æ”¯æŒç±»å‹åˆ¤æ–­
        if any(word in content for word in ['è¡¥è´´', 'èµ„åŠ©', 'å¥–åŠ±', 'æ‰¶æŒèµ„é‡‘']):
            if any(word in content for word in ['æ— å¿', 'ä¸äºˆè¿”è¿˜']):
                policy_type = 'grant'
            else:
                policy_type = 'subsidy'
        elif any(word in content for word in ['ç¨æ”¶ä¼˜æƒ ', 'å‡ç¨', 'å…ç¨']):
            policy_type = 'tax'
        elif any(word in content for word in ['è´·æ¬¾', 'èèµ„', 'æ‹…ä¿']):
            policy_type = 'loan'
        elif any(word in content for word in ['æŠ•èµ„', 'è‚¡æƒ', 'åŸºé‡‘']):
            policy_type = 'investment'
        else:
            policy_type = 'subsidy'
        
        # æå–é‡‘é¢
        amount_patterns = [
            r'æœ€é«˜[ä¸]?è¶…è¿‡?(\d+)ä¸‡å…ƒ',
            r'ä¸è¶…è¿‡(\d+)ä¸‡å…ƒ',
            r'ç»™äºˆ(\d+)ä¸‡å…ƒ',
            r'èµ„åŠ©(\d+)ä¸‡å…ƒ',
            r'è¡¥è´´(\d+)ä¸‡å…ƒ',
            r'å¥–åŠ±(\d+)ä¸‡å…ƒ'
        ]
        
        max_amount = 1000000  # é»˜è®¤100ä¸‡
        
        for pattern in amount_patterns:
            matches = re.findall(pattern, content)
            if matches:
                try:
                    amount_wan = int(matches[0])
                    max_amount = amount_wan * 10000
                    break
                except:
                    continue
        
        return policy_type, max_amount
    
    def extract_requirements(self, content: str) -> List[str]:
        """æå–ç”³è¯·æ¡ä»¶"""
        requirements = []
        
        # å¯»æ‰¾æ¡ä»¶ç›¸å…³æ®µè½
        condition_patterns = [
            r'ç”³è¯·æ¡ä»¶[ï¼š:](.*?)(?=[ã€‚\n]|$)',
            r'æ”¯æŒå¯¹è±¡[ï¼š:](.*?)(?=[ã€‚\n]|$)',
            r'é€‚ç”¨èŒƒå›´[ï¼š:](.*?)(?=[ã€‚\n]|$)',
            r'æ”¯æŒæ¡ä»¶[ï¼š:](.*?)(?=[ã€‚\n]|$)'
        ]
        
        for pattern in condition_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                # åˆ†å‰²æ¡ä»¶
                conditions = re.split(r'[ï¼›;ï¼Œ,]', match)
                for condition in conditions:
                    condition = condition.strip()
                    if len(condition) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ¡ä»¶
                        requirements.append(condition)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“æ¡ä»¶ï¼Œæ·»åŠ é€šç”¨æ¡ä»¶
        if not requirements:
            if 'å¾æ±‡åŒº' in content:
                requirements.append('ä¼ä¸šæ³¨å†Œåœ°åœ¨å¾æ±‡åŒº')
            if 'äººå·¥æ™ºèƒ½' in content:
                requirements.append('ä»äº‹äººå·¥æ™ºèƒ½ç›¸å…³ä¸šåŠ¡')
            requirements.append('ä¼ä¸šä¿¡ç”¨çŠ¶å†µè‰¯å¥½')
            requirements.append('ç¬¦åˆå›½å®¶äº§ä¸šæ”¿ç­–')
        
        return requirements[:6]  # æœ€å¤šè¿”å›6ä¸ªæ¡ä»¶
    
    def extract_industry_tags(self, title: str, content: str) -> List[str]:
        """æå–è¡Œä¸šæ ‡ç­¾"""
        tags = []
        text = title + ' ' + content
        
        # è¡Œä¸šå…³é”®è¯æ˜ å°„
        industry_keywords = {
            'äººå·¥æ™ºèƒ½': ['äººå·¥æ™ºèƒ½', 'AI', 'æ™ºèƒ½', 'ç®—æ³•', 'å¤§æ¨¡å‹'],
            'ç§‘æŠ€åˆ›æ–°': ['ç§‘æŠ€', 'åˆ›æ–°', 'ç ”å‘', 'æŠ€æœ¯'],
            'åˆ¶é€ ä¸š': ['åˆ¶é€ ', 'å·¥ä¸š', 'ç”Ÿäº§'],
            'æ•°å­—ç»æµ': ['æ•°å­—åŒ–', 'ä¿¡æ¯åŒ–', 'äº’è”ç½‘'],
            'ç”Ÿç‰©åŒ»è¯': ['ç”Ÿç‰©', 'åŒ»è¯', 'åŒ»ç–—'],
            'æ–°èƒ½æº': ['æ–°èƒ½æº', 'æ¸…æ´èƒ½æº', 'èŠ‚èƒ½'],
            'é‡‘èç§‘æŠ€': ['é‡‘èç§‘æŠ€', 'fintech', 'æ”¯ä»˜'],
            'æ–‡åˆ›äº§ä¸š': ['æ–‡åŒ–', 'åˆ›æ„', 'è®¾è®¡'],
            'å°å¾®ä¼ä¸š': ['å°å¾®ä¼ä¸š', 'ä¸­å°ä¼ä¸š'],
            'ä½æˆ¿ä¿éšœ': ['ä½æˆ¿', 'ä¿éšœæˆ¿', 'å®‰å±…']
        }
        
        for tag, keywords in industry_keywords.items():
            if any(keyword in text for keyword in keywords):
                tags.append(tag)
        
        return tags[:5]  # æœ€å¤šè¿”å›5ä¸ªæ ‡ç­¾
    
    def crawl_all_policies(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰æ”¿ç­–"""
        print("ğŸš€ å¼€å§‹çˆ¬å–çœŸå®æ”¿ç­–æ•°æ®...")
        policies = []
        
        for i, policy_info in enumerate(self.policy_urls):
            print(f"ğŸ“„ [{i+1}/{len(self.policy_urls)}] {policy_info['title']}")
            
            policy_data = self.crawl_policy_content(policy_info['url'])
            if policy_data:
                # åˆå¹¶åŸºç¡€ä¿¡æ¯
                policy_data.update({
                    'policy_id': f"XH2024{i+1:03d}",
                    'region': policy_info['region'],
                    'category': policy_info['category']
                })
                policies.append(policy_data)
                print(f"âœ… æˆåŠŸçˆ¬å–: {policy_data['title']}")
            else:
                print(f"âŒ å¤±è´¥: {policy_info['title']}")
            
            # å»¶æ—¶é¿å…è¢«å°
            time.sleep(2)
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(policies)} æ¡æ”¿ç­–")
        return policies
    
    def save_policies(self, policies: List[Dict], filename: str = "real_policies.json"):
        """ä¿å­˜æ”¿ç­–æ•°æ®"""
        os.makedirs("data", exist_ok=True)
        filepath = os.path.join("data", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(policies, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ”¿ç­–æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")

def main():
    """ä¸»å‡½æ•°"""
    crawler = RealPolicyCrawler()
    
    # çˆ¬å–æ‰€æœ‰æ”¿ç­–
    policies = crawler.crawl_all_policies()
    
    # ä¿å­˜æ•°æ®
    crawler.save_policies(policies)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  æ€»æ”¿ç­–æ•°: {len(policies)}")
    print(f"  AIç›¸å…³æ”¿ç­–: {len([p for p in policies if 'äººå·¥æ™ºèƒ½' in p.get('title', '')])}")
    print(f"  èµ„é‡‘ç±»æ”¿ç­–: {len([p for p in policies if any(word in p.get('title', '') for word in ['èµ„é‡‘', 'è¡¥è´´', 'å¥–åŠ±'])])}")
    
    return policies

if __name__ == "__main__":
    main() 